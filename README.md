# Ops Workflow Automation Platform  
**End-to-End Data Ingestion, Validation, Orchestration & Analytics Pipeline**

---

## ğŸ“Œ Overview

This project is a **production-grade data workflow automation platform** designed to simulate how modern analytics teams ingest, validate, version, orchestrate, and analyze data at scale.

It combines **FastAPI**, **PostgreSQL**, **Apache Airflow**, **Streamlit**, and **Docker** to deliver a fully automated, analyst-friendly data pipeline with strong emphasis on **data quality, reliability, and reproducibility**.

The system is built to mirror **real-world data engineering & analytics workflows** used in fintech, SaaS, and product analytics teams.

---

## ğŸ¯ Key Objectives

- Ensure **high-quality, trusted data** through automated validation
- Enable **idempotent & incremental data loads**
- Maintain **dataset versioning & metadata tracking**
- Provide **self-serve data quality dashboards** for analysts
- Orchestrate workflows reliably using **Airflow**
- Reduce manual operations, latency, and data errors

---

## ğŸ—ï¸ Architecture

Streamlit UI
|
| (CSV Upload)
v
FastAPI (Upload & Validation Service)
|
|-- Schema Validation
|-- Deduplication
|-- Dataset Versioning
|-- Data Quality Metrics
|
v
PostgreSQL (Analytics Warehouse)
|
v
Apache Airflow (Workflow Orchestration)
|
v
Downstream Transformations & Loads



All services run in **isolated Docker containers** using `docker-compose`.

---

## âš™ï¸ Tech Stack

| Layer | Technology |
|------|-----------|
Backend API | **FastAPI**
Database | **PostgreSQL**
Workflow Orchestration | **Apache Airflow**
Frontend Dashboard | **Streamlit**
Data Processing | **Pandas**
Containerization | **Docker & Docker Compose**
ORM | **SQLAlchemy**

---

## ğŸš€ Features

### âœ… Data Ingestion
- Upload raw CSV datasets via Streamlit UI
- FastAPI handles ingestion with schema enforcement

### âœ… Data Validation & Quality Checks
- Schema validation
- Null percentage calculation
- Duplicate row detection
- Checksum-based deduplication

### âœ… Dataset Versioning & Metadata
- Automatic dataset versioning per filename
- Metadata tracking:
  - Row counts
  - Checksums
  - Upload timestamps

### âœ… Incremental & Idempotent Loads
- Prevents duplicate inserts
- Safe re-runs without corrupting data
- Ensures consistent reporting

### âœ… Workflow Orchestration
- Automatically triggers Airflow DAG on successful upload
- Enables downstream transformations & analytics jobs

### âœ… Data Quality Dashboard
- Streamlit dashboard showing:
  - Null percentages
  - Duplicate rows
  - Dataset freshness
  - Upload history
- Built for analysts & non-technical stakeholders

---

## ğŸ“‚ Project Structure

ops_workflow_automation/
â”‚
â”œâ”€â”€ api/
â”‚ â””â”€â”€ upload_service.py # FastAPI backend
â”‚
â”œâ”€â”€ dag/
â”‚ â””â”€â”€ ops_workflow_automation_dag.py
â”‚
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ ops_uploader.py # Streamlit dashboard
â”‚
â”œâ”€â”€ raw_data/ # Sample input data
â”‚
â”œâ”€â”€ validation.py # Data validation logic
â”œâ”€â”€ ingest.py
â”œâ”€â”€ transformation.py
â”œâ”€â”€ load_to_postgres.py
â”œâ”€â”€ create_table.sql # DB schema
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



---

## ğŸ§ª How the Pipeline Works (Step-by-Step)

1. User uploads a CSV via **Streamlit UI**
2. FastAPI:
   - Validates schema
   - Computes data quality metrics
   - Checks for duplicates using checksums
3. Metadata & quality metrics stored in PostgreSQL
4. Data loaded incrementally into analytics tables
5. Airflow DAG is auto-triggered for downstream jobs
6. Analysts monitor data quality via Streamlit dashboard

---

## ğŸ› ï¸ Setup Instructions

### Prerequisites
- Docker & Docker Compose
- Git

---

### Clone the Repository
```bash
git clone https://github.com/Advait-sankar/Ops_workflow_automation.git
cd Ops_workflow_automation
```
### â–¶ï¸ Start the Platform

```bash
docker-compose up --build
```

### ğŸ“Š Use Cases
1. Data Analyst: Monitor data quality, freshness, and anomalies
2. Business Analyst: Trust metrics, dashboards, and reporting inputs
3. Product Analyst: Reliable experiment data and KPI pipelines
4. Data Engineer: Orchestrate, monitor, and scale workflows


### ğŸ“ˆ Why This Project Matters
This project demonstrates:
    1. Real-world analytics engineering practices
    2. Strong data quality ownership and validation
    3. Production-ready workflow orchestration with Airflow
    4. Analyst-first, self-serve dashboards
    5. End-to-end system thinking, not just scripts
It is intentionally designed to align with Data Analyst, Business Analyst, and Product Analyst roles in modern data-driven organizations.

